{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a986e77-a33d-4b9a-8e87-281743194af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from token_shap import TokenSHAP\n",
    "from nltk.corpus import words\n",
    "from termcolor import colored\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4de0ae5-8b41-49c4-97a2-71b6e01f789f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ronig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e242163-8d32-4ff2-a2b0-79a32fd4c323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inject_random_words(prompts, injection_rate=(0.2, 0.3)):\n",
    "    word_list = words.words()\n",
    "    injected_prompts = []\n",
    "    dict_injected = {}\n",
    "    for prompt in prompts:\n",
    "        words_in_prompt = prompt.split()\n",
    "        num_injections = int(len(words_in_prompt) * random.uniform(*injection_rate))\n",
    "        injection_indices = random.sample(range(len(words_in_prompt) + 1), num_injections)\n",
    "        random_words = []\n",
    "        for index in sorted(injection_indices, reverse=True):\n",
    "            random_word = random.choice(word_list)\n",
    "            words_in_prompt.insert(index, random_word)\n",
    "            random_words.append(random_word)\n",
    "        injected_prompts.append(' '.join(words_in_prompt))\n",
    "        dict_injected[prompt] = random_words\n",
    "    return injected_prompts, dict_injected\n",
    "\n",
    "def color_injected_words(original_prompts, injected_prompts, n):\n",
    "    for _ in range(n):\n",
    "        idx = random.randint(0, len(original_prompts) - 1)\n",
    "        original_words = set(original_prompts[idx].split())\n",
    "        injected_words = injected_prompts[idx].split()\n",
    "        \n",
    "        colored_prompt = []\n",
    "        for word in injected_words:\n",
    "            if word not in original_words:\n",
    "                colored_prompt.append(colored(word, 'red'))\n",
    "            else:\n",
    "                colored_prompt.append(word)\n",
    "        \n",
    "        print(' '.join(colored_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1984902a-b364-4778-9265-552323edad96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"tatsu-lab/alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd5a93b-8edc-4b35-83f4-5360ce7d79b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = random.sample(ds['train']['instruction'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07be018-e3f2-4799-9bbd-418a2547f21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "injected_prompts, dict_injected = inject_random_words(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86b9043-8330-4752-a6db-fb0fda6a66fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mbinotic\u001b[0m Generate a list of five popular streaming subscription services. \u001b[31mfusteric\u001b[0m\n",
      "How many \u001b[31mspirometric\u001b[0m countries make up the European Union?\n",
      "\u001b[31mplayfulness\u001b[0m Identify the type of the function y = x^2 \u001b[31mfranticly\u001b[0m + 3\n",
      "What is the legal \u001b[31mtripsill\u001b[0m principle behind copyright \u001b[31mbiting\u001b[0m law?\n",
      "Explain how infectious disease \u001b[31mGraptoloidea\u001b[0m spreads\n",
      "Output the name of \u001b[31mcaliduct\u001b[0m the day \u001b[31mpatriarchy\u001b[0m of the week \u001b[31mJennifer\u001b[0m for \u001b[31mretroiridian\u001b[0m a given date in MM/DD/YYYY format.\n",
      "\u001b[31mdacryocystotomy\u001b[0m List 5 popular dishes in US.\n",
      "Give me \u001b[31mmedically\u001b[0m a sentence to \u001b[31mheyday\u001b[0m describe the feeling of joy.\n",
      "Suggest a \u001b[31mcartographic\u001b[0m suitable input to the following instruction.\n",
      "Generate a question \u001b[31munhastened\u001b[0m about the immune system\n"
     ]
    }
   ],
   "source": [
    "color_injected_words(prompts, injected_prompts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34f22153-0033-447a-9256-bed66d4f65dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/opensora/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the landmark built in 1060 near Athens.\n",
      "Generate an animated gif with an astronaut sailing in a spaceship\n",
      "Create a mnemonic device to remember the following words\n",
      "Create a binary classification query which determines whether a given article talks about Covid-19.\n",
      "Suggest a suitable input to the following instruction.\n",
      "Compare the terms 'sublimation' and 'deposition'.\n",
      "Generate a list of five popular streaming subscription services.\n",
      "Deleted the second-to-last sentence of this paragraph.\n",
      "Construct a for loop to count from 1 to 10.\n",
      "Name two endangered species of plants and two endangered species of animals.\n",
      "Write a short story about a computer that can predict the future.\n",
      "Given an article, identify the main author's point of view.\n",
      "Given a suitable input, generate a poem that captures the emotion of happiness.\n",
      "Organize a list of tasks in chronological order.\n",
      "Paraphrase the following sentence to emphasize the main idea.\n",
      "Which chess piece moves in an \"L\" shape?\n",
      "Cite three references for the given topic.\n",
      "Identify the main theme of the following poem and provide a brief analysis.\n",
      "Write a function that adds two numbers\n",
      "Find two songs that I could add to a playlist for motivation.\n",
      "Write a regular expression to match a string of 10 numerical digits.\n",
      "Find the sentiment associated with the following statement.\n",
      "Explain the physiological process of breathing.\n",
      "Write an essay about the recent advances in artificial intelligence and its implications.\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "Response ended prematurely",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/opensora/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opensora/lib/python3.9/site-packages/urllib3/response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opensora/lib/python3.9/site-packages/urllib3/response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opensora/lib/python3.9/site-packages/urllib3/response.py:1136\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;66;03m# Truncated at start of next chunk\u001b[39;00m\n\u001b[0;32m-> 1136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse ended prematurely\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mProtocolError\u001b[0m: Response ended prematurely",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[0;32m---> 25\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     original_shap_values[prompt] \u001b[38;5;241m=\u001b[39m tshap\u001b[38;5;241m.\u001b[39mshapley_values\n\u001b[1;32m     27\u001b[0m     save_shap_values(original_shap_values, save_path)\n",
      "File \u001b[0;32m/srv/jupyterhub/research/ronig/TokenSHAP/token_shap.py:209\u001b[0m, in \u001b[0;36manalyze\u001b[0;34m(self, prompt, splitter, sampling_ratio, print_result)\u001b[0m\n\u001b[1;32m    206\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m    207\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt, splitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampling_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, print_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    210\u001b[0m     baseline_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_baseline(prompt)\n\u001b[1;32m    211\u001b[0m     token_combinations_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_result_per_token_combination(prompt, splitter, sampling_ratio)\n",
      "File \u001b[0;32m/srv/jupyterhub/research/ronig/TokenSHAP/token_shap.py:73\u001b[0m, in \u001b[0;36mTokenSHAP._get_result_per_token_combination\u001b[0;34m(self, prompt, splitter, sampling_ratio)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         text \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39mjoin(combination)\n\u001b[0;32m---> 73\u001b[0m     text_response, response \u001b[38;5;241m=\u001b[39m \u001b[43minteract_with_ollama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mollama_api\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     prompt_responses[text] \u001b[38;5;241m=\u001b[39m text_response\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_responses\n",
      "File \u001b[0;32m/srv/jupyterhub/research/ronig/TokenSHAP/ollama_interact.py:74\u001b[0m, in \u001b[0;36minteract_with_ollama\u001b[0;34m(prompt, messages, image_path, model, stream, output_handler, api_url)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m full_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line:\n\u001b[1;32m     76\u001b[0m         json_response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opensora/lib/python3.9/site-packages/requests/models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    870\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    871\u001b[0m ):\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opensora/lib/python3.9/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: Response ended prematurely"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize TokenSHAP with your model & tokenizer\n",
    "model_name = \"llama3\"\n",
    "tshap = TokenSHAP(model_name, tokenizer_path=\"NousResearch/Hermes-2-Theta-Llama-3-8B\")\n",
    "\n",
    "# Path to save SHAP values\n",
    "save_path = \"shap_values.json\"\n",
    "\n",
    "# Load existing SHAP values if the file exists\n",
    "if os.path.exists(save_path):\n",
    "    with open(save_path, 'r') as f:\n",
    "        original_shap_values = json.load(f)\n",
    "else:\n",
    "    original_shap_values = {}\n",
    "\n",
    "# Function to save SHAP values to disk\n",
    "def save_shap_values(shap_values, save_path):\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(shap_values, f)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    results = tshap.analyze(prompt, sampling_ratio=0.2, splitter=' ')\n",
    "    original_shap_values[prompt] = tshap.shapley_values\n",
    "    save_shap_values(original_shap_values, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73273a2-b97f-4ee6-9def-689af75ea22c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/opensora/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreason Describe the different flavors of the following ice cream: tutor\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize TokenSHAP with your model & tokenizer\n",
    "model_name = \"llama3\"\n",
    "tshap = TokenSHAP(model_name, tokenizer_path = \"NousResearch/Hermes-2-Theta-Llama-3-8B\")\n",
    "injected_shap_values = {}\n",
    "for prompt in injected_prompts:\n",
    "    print(prompt)\n",
    "    results = tshap.analyze(prompt, sampling_ratio = 0.2, splitter = ' ')\n",
    "    injected_shap_values[prompt] = tshap.shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76bd27-51d9-4eae-a7b5-e43a598c3eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "all_words = defaultdict(list)\n",
    "for prompt_dict in original_shap_values.values():\n",
    "    for word, value in prompt_dict.items():\n",
    "        all_words[word].append(value)\n",
    "for prompt_dict in injected_shap_values.values():\n",
    "    for word, value in prompt_dict.items():\n",
    "        all_words[word].append(value)\n",
    "\n",
    "word_shap =  {word: np.mean(values) for word, values in all_words.items()}\n",
    "word_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b455ff-fbc5-4398-8672-77b9121b09f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "word_freq = defaultdict(int)\n",
    "for prompt in prompts + injected_prompts:\n",
    "    for word in prompt.split():\n",
    "        word_freq[word] += 1\n",
    "\n",
    "injected_words = set([word for words in dict_injected.values() for word in words])\n",
    "word_correlation = {}\n",
    "for word in word_shap.keys():\n",
    "    in_injected = sum(1 for prompt in injected_prompts if word in prompt.split())\n",
    "    in_original = sum(1 for prompt in prompts if word in prompt.split())\n",
    "    word_correlation[word] = (in_injected / len(injected_prompts)) - (in_original / len(prompts))\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'word': list(word_shap.keys()),\n",
    "    'shap_value': list(word_shap.values()),\n",
    "    'correlation': [word_correlation.get(word, 0) for word in word_shap.keys()],\n",
    "    'frequency': [word_freq.get(word, 0) for word in word_shap.keys()],\n",
    "    'is_injected': [word in injected_words for word in word_shap.keys()]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b023538-2fc5-4e2c-bbcd-ba5e83bfc320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "low_importance_words = set(results[results['shap_value'] < low_importance_threshold]['word'])\n",
    "injected_words = set([word for words in dict_injected.values() for word in words])\n",
    "\n",
    "low_importance_injected = low_importance_words.intersection(injected_words)\n",
    "print(f\"Number of low importance words that are also injected: {len(low_importance_injected)}\")\n",
    "print(f\"Percentage of injected words that are low importance: {len(low_importance_injected) / len(injected_words) * 100:.2f}%\")\n",
    "\n",
    "correlation_matrix = np.corrcoef(results['shap_value'], results['is_injected'])\n",
    "print(f\"Correlation coefficient between SHAP values and being an injected word: {correlation_matrix[0, 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b9d8f-0137-4ed9-b00f-fcff4cdccf5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "low_importance_threshold=0.1\n",
    "low_importance_words = results[results['shap_value'] < low_importance_threshold]\n",
    "    \n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(low_importance_words['shap_value'], \n",
    "                      low_importance_words['correlation'],\n",
    "                      c=low_importance_words['frequency'], \n",
    "                      cmap='viridis', \n",
    "                      s=low_importance_words['frequency'], \n",
    "                      alpha=0.6)\n",
    "plt.colorbar(scatter, label='Frequency')\n",
    "plt.xlabel('SHAP Value')\n",
    "plt.ylabel('Correlation with Injected Words')\n",
    "plt.title('Low Importance Words: SHAP Value vs. Correlation with Injected Words')\n",
    "\n",
    "for _, row in low_importance_words.nlargest(10, 'correlation').iterrows():\n",
    "    plt.annotate(row['word'], (row['shap_value'], row['correlation']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "heatmap_data = top_low_importance[['shap_value', 'correlation', 'frequency']].astype(float)\n",
    "heatmap_data['is_injected'] = top_low_importance['is_injected'].astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data.set_index(top_low_importance['word']), \n",
    "            annot=True, cmap='YlOrRd', fmt='.2f')\n",
    "plt.title('Top 20 Low Importance Words by Correlation with Injected Words')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSora",
   "language": "python",
   "name": "opensora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
